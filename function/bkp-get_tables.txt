import psycopg2
import apache_beam as beam
import logging
import pandas as pd
from google.cloud import storage
from google.cloud import secretmanager
import os
import json

class GetTables(beam.DoFn):
    def process(self, element):
        for table in element:
            # Parâmetros de conexão
            # Cria um cliente
            client = secretmanager.SecretManagerServiceClient()

            # Monta o nome do recurso do secret
            name = "projects/1030074550193/secrets/secret-motors-word-db/versions/1"

            # Acessa o valor do secret
            response = client.access_secret_version(name=name)

            # Decode o payload
            secret_payload = response.payload.data.decode("UTF-8")

            # Se o payload for um JSON, converta para um dicionário
            secret_dict = json.loads(secret_payload)

            # Definindo as variáveis de ambiente
            os.environ['DB_USER'] = secret_dict['DB_USER']
            os.environ['DB_PASSWORD'] = secret_dict['DB_PASSWORD']
            os.environ['DB_HOST'] = secret_dict['DB_HOST']
            os.environ['DB_PORT'] = secret_dict['DB_PORT']
            os.environ['DB_DATABASE'] = secret_dict['DB_DATABASE']
            
            # Pegando as credenciais via variável de ambiente
            USERNAME = os.getenv('DB_USER')
            PASSWORD = os.getenv('DB_PASSWORD')
            HOST = os.getenv('DB_HOST')
            PORT = os.getenv('DB_PORT')
            DATABASE = os.getenv('DB_DATABASE')

            conn_params = {
                "host": HOST,
                "database": DATABASE,
                "user": USERNAME,
                "password": PASSWORD,
                "port": PORT
            }
            try:
                # Conectar ao banco de dados
                conn = psycopg2.connect(**conn_params)
                
                # Criar um cursor para executar consultas SQL
                cursor = conn.cursor()

                query = f"select * from {table}"

                cursor.execute(query)

                col_names = [desc[0] for desc in cursor.description]

                rows = cursor.fetchall()

                df_new = pd.DataFrame(rows, columns=col_names)
                
                cursor.close()
                conn.close()                

                # Nome do bucket e caminho do arquivo
                bucket_name = 'motors-word'
                path = f'landing/{table}.parquet'

                client = storage.Client()
                bucket = client.get_bucket(bucket_name)
                blob = bucket.blob(path)

                # Verificar se o arquivo já existe
                if blob.exists():
                    # Ler o arquivo Parquet existente do GCS para um DataFrame
                    gcs_file_path = f'gs://{bucket_name}/{path}'
                    df_old = pd.read_parquet(gcs_file_path)

                    # Fazer o merge dos DataFrames
                    df_combined = pd.concat([df_old, df_new]).drop_duplicates().reset_index(drop=True)
                else:
                    df_combined = df_new

                # Salvar o DataFrame combinado como Parquet
                parquet = df_combined.to_parquet(index=False)
                blob.upload_from_string(parquet, content_type='application/octet-stream')

                logging.info(f'{table} {"=" * (80 - len(table))} {df_combined.shape}')

                yield parquet

            except psycopg2.Error as e:
                logging.info(f"Erro encontrado durante a conexão: {e}")
